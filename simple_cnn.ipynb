{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import TensorBoard,History,EarlyStopping,CSVLogger\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train'\n",
    "validation_path = './validation'\n",
    "test_path = './test'\n",
    "assert(os.path.exists(train_path))\n",
    "assert(os.path.exists(validation_path))\n",
    "assert(os.path.exists(test_path))\n",
    "\n",
    "nb_train_samples = 20000\n",
    "nb_val_samples = 2424\n",
    "image_size = (224,224)\n",
    "batch_size = 128\n",
    "class_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_CNN(input_shape=(224,224,3),class_num=10):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(3,3),input_shape=input_shape,activation='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPool2D()) \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(class_num,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adadelta(),metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(img_path):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,test_path,csv='sample_submission.csv'):\n",
    "    columns = ['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    file_list = os.listdir(test_path)\n",
    "    for i,file in enumerate(file_list):\n",
    "        test_data = image_preprocess(test_path + '/' + file)\n",
    "        y_pred = model.predict(test_data,batch_size=1,verbose=0)\n",
    "        y_pred = np.clip(y_pred,0.001,0.999)\n",
    "        y_pred = y_pred[0].tolist()\n",
    "        df.loc[i] = [file] + y_pred\n",
    "\n",
    "        \n",
    "    df.to_csv(csv,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 10 classes.\n",
      "Found 2424 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_path,\n",
    "                                    target_size=image_size,\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                                    validation_path,\n",
    "                                    target_size=image_size,\n",
    "                                    batch_size=batch_size,                                    \n",
    "                                    class_mode='categorical',\n",
    "                                    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 222, 222, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 109, 109, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 109, 109, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 52, 52, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               11075840  \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 11,108,586\n",
      "Trainable params: 11,107,818\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_simple_CNN()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 102s 655ms/step - loss: 0.3214 - acc: 0.9124 - val_loss: 2.5789 - val_acc: 0.2556\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 107s 686ms/step - loss: 0.0180 - acc: 0.9965 - val_loss: 3.7387 - val_acc: 0.1890\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 107s 685ms/step - loss: 0.0050 - acc: 0.9994 - val_loss: 4.3296 - val_acc: 0.1903\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 107s 686ms/step - loss: 0.0017 - acc: 0.9999 - val_loss: 3.9433 - val_acc: 0.1947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60e377bd68>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=10,\n",
    "                    callbacks=[EarlyStopping(patience=3)],\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 10 classes.\n",
      "Found 2424 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = image.ImageDataGenerator(rescale=1./255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         rotation_range=15,\n",
    "                                         height_shift_range=0.2,\n",
    "                                         width_shift_range=0.2,\n",
    "                                         channel_shift_range=10,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "validation_datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_path,\n",
    "                                    target_size=image_size,\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical',\n",
    "                                    shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                                    validation_path,\n",
    "                                    target_size=image_size,\n",
    "                                    batch_size=batch_size,                                    \n",
    "                                    class_mode='categorical',\n",
    "                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 327s 2s/step - loss: 2.5182 - acc: 0.2018 - val_loss: 3.5353 - val_acc: 0.1033\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 327s 2s/step - loss: 1.9234 - acc: 0.3314 - val_loss: 2.1121 - val_acc: 0.3062\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 327s 2s/step - loss: 1.5971 - acc: 0.4332 - val_loss: 1.7577 - val_acc: 0.4617\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 325s 2s/step - loss: 1.3923 - acc: 0.5111 - val_loss: 2.2322 - val_acc: 0.4260\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 323s 2s/step - loss: 1.1895 - acc: 0.5850 - val_loss: 3.3588 - val_acc: 0.2652\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 322s 2s/step - loss: 1.0417 - acc: 0.6358 - val_loss: 2.2036 - val_acc: 0.3968\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 323s 2s/step - loss: 0.9188 - acc: 0.6869 - val_loss: 1.2740 - val_acc: 0.6307\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 319s 2s/step - loss: 0.8004 - acc: 0.7291 - val_loss: 1.2944 - val_acc: 0.5688\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 322s 2s/step - loss: 0.7337 - acc: 0.7517 - val_loss: 1.7677 - val_acc: 0.5501\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 321s 2s/step - loss: 0.6677 - acc: 0.7781 - val_loss: 2.1275 - val_acc: 0.5113\n"
     ]
    }
   ],
   "source": [
    "model = build_simple_CNN()\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [3.535255399015215, 2.112108870250423, 1.7576891528604752, 2.2322484383599686, 3.358825390438066, 2.203625086707936, 1.2739743555049032, 1.294378822688857, 1.7676885667993631, 2.1275294200883925], 'val_acc': [0.1032986111111111, 0.30618466898954705, 0.4616724738675958, 0.4259581881533101, 0.2652439024390244, 0.3967770041073656, 0.6306620203867191, 0.5688153309585325, 0.5500871080918179, 0.5113240418767472], 'loss': [2.5182303343063746, 1.9220908062357733, 1.5976088434793716, 1.3910034299856606, 1.1884645793748938, 1.0432066741772896, 0.9187738998578145, 0.799754138806877, 0.733280458887994, 0.6678156073350645], 'acc': [0.20182291666666666, 0.33206917475728154, 0.43307165861513686, 0.5114231078904992, 0.5849436392914654, 0.6355173107890499, 0.6869491185897436, 0.7297734627831716, 0.7519625603864735, 0.7782306763285024]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('simple_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 327s 2s/step - loss: 0.6065 - acc: 0.7965 - val_loss: 1.1753 - val_acc: 0.6944\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 329s 2s/step - loss: 0.5834 - acc: 0.8044 - val_loss: 1.0163 - val_acc: 0.7012\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 326s 2s/step - loss: 0.5800 - acc: 0.8074 - val_loss: 0.9508 - val_acc: 0.7169\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 325s 2s/step - loss: 0.5720 - acc: 0.8086 - val_loss: 1.2023 - val_acc: 0.6459\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 324s 2s/step - loss: 0.5667 - acc: 0.8109 - val_loss: 1.2925 - val_acc: 0.6476\n"
     ]
    }
   ],
   "source": [
    "model = load_model('simple_cnn_model.h5')\n",
    "\n",
    "sgd = SGD(lr=0.0001, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=5,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 323s 2s/step - loss: 0.5625 - acc: 0.8124 - val_loss: 1.2501 - val_acc: 0.6840\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 329s 2s/step - loss: 0.5663 - acc: 0.8098 - val_loss: 1.1280 - val_acc: 0.6546\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 329s 2s/step - loss: 0.5598 - acc: 0.8173 - val_loss: 1.2450 - val_acc: 0.6320\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 326s 2s/step - loss: 0.5503 - acc: 0.8181 - val_loss: 1.2443 - val_acc: 0.6416\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 324s 2s/step - loss: 0.5519 - acc: 0.8145 - val_loss: 1.1302 - val_acc: 0.6847\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=5,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('simple_cnn_final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('simple_cnn_model.h5')\n",
    "\n",
    "test_model(model,test_path,csv='simple_cnn_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
